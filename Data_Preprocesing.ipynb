{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "lpXV47-4ifQV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuLyLxdtiej6"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# STEP 1: IMPORT REQUIRED LIBRARIES\n",
        "# ================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "\n",
        "# ================================\n",
        "# STEP 2: LOAD THE EXCEL FILE\n",
        "# ================================\n",
        "file_path = '/content/Crop_recommendation.csv'  # Update this if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# ================================\n",
        "# STEP 3: CHECK FOR MISSING VALUES\n",
        "# ================================\n",
        "print(\"üîç Missing values before imputation:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# ================================\n",
        "# STEP 4: HANDLE MISSING VALUES (Median Imputation)\n",
        "# ================================\n",
        "# Columns to check: Soil nutrients (N, P, K), pH, temperature, humidity, rainfall\n",
        "cols_to_impute = ['N', 'P', 'K', 'ph', 'temperature', 'humidity', 'rainfall']\n",
        "\n",
        "for col in cols_to_impute:\n",
        "    median_val = df[col].median()\n",
        "    df[col].fillna(median_val, inplace=True)\n",
        "    print(f\"‚úÖ Filled missing values in '{col}' with median: {median_val}\")\n",
        "\n",
        "print(\"\\n‚úÖ All missing values filled.\\n\")\n",
        "\n",
        "# ================================\n",
        "# STEP 5: FEATURE SCALING (Min-Max Normalization)\n",
        "# ================================\n",
        "scaler = MinMaxScaler()\n",
        "df[cols_to_impute] = scaler.fit_transform(df[cols_to_impute])\n",
        "\n",
        "print(\"‚úÖ Features normalized using Min-Max Scaling:\")\n",
        "print(df[cols_to_impute].head())\n",
        "\n",
        "# ================================\n",
        "# STEP 6: ENCODE CATEGORICAL LABELS (Crop Types)\n",
        "# ================================\n",
        "\n",
        "# Method 1: Label Encoding (for classification models)\n",
        "le = LabelEncoder()\n",
        "df['label_encoded'] = le.fit_transform(df['label'])\n",
        "\n",
        "# Optional: Save mapping for reference\n",
        "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(\"\\n‚úÖ Crop label encoded mapping:\")\n",
        "print(label_mapping)\n",
        "\n",
        "# Method 2: One-Hot Encoding (for analysis/ML models that benefit from OHE)\n",
        "df_onehot = pd.get_dummies(df, columns=['label'], prefix='crop')\n",
        "\n",
        "# Display result\n",
        "print(\"\\nüìä Final preprocessed dataset (head):\")\n",
        "print(df_onehot.head())\n",
        "\n",
        "# ================================\n",
        "# (Optional) Save Preprocessed Data\n",
        "# ================================\n",
        "# df_onehot.to_csv('/content/preprocessed_crop_data.csv', index=False)\n"
      ]
    }
  ]
}